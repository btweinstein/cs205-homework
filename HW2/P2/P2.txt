>>Characterize the performance of each approach (coarse, medium, and fine-grained locking)
on the two cases in P2.py: random data exchanges and correlated data exchanges.

Let us list the running times of each scenario first.

#### Uncorrelated ####
serial: 0.20 seconds
fine grained: 3.67 seconds
medium grained: I plotted this as a function of N. See "uncorrelated_vs_N.png". Times range from 6.5 seconds to 17
                seconds depending on N.

#### Correlated ####
serial: 0.21 seconds
fine grained: 2.56 seconds
medium grained: See "correlated_vs_N.png". Times range from 4 seconds to about 10 seconds depending on N.

Let us now characterize the performance of each approach.

1. Fine Grained

a) Uncorrelated

In the fine grained case, we have a lock for each element in the counts array. The idea here is that the overhead
associated with grabbing, releasing, and waiting for a lock will be offset by the speedup of 4 simultaneous threads
moving data around. Unfortunately, the fine grained uncorrelated code runs almost 10 times slower than the serial
uncorrelated case! Evidently, the penalty of using locks is greater than the gain of using 4 simultaneous threads.

b) Correlated

The correlated fine-grained code runs *faster* than the uncorrelated code. This is surprising to me...naively, I would
expect that if the source and destination are correlated when transferring data, there would be more contention for locks.
This does not appear to be the case. I think this must be attributed to OpenMP and compiler strageness.

2. Medium Grained

a) Uncorrelated

Now, there are less locks; a single lock controls access to multiple array elements. Naively, this could be a good
choice as it will lessen the penalty of assigning and releasing many locks. It will only speed up the code if there
is not a lot of contention for the locks, i.e. if N is too big, there will be too much contention for locks and the
code will slow down.

See plot "uncorrelated_vs_N.png". Strangely, when N=1, we record a time of roughly 9 seconds. N=1 corresponds to the
fine-grained case where we recorded a time of roughly 2.56 seconds; evidently the overhead associated with assigning
locks is large (i.e. doing division and addition). I again attribute this to compiler strangeness.

Regardless, the trends we see in the plot make sense; there is a peak in time around N=3; the combination of many locks
and increased contention for locks must create this. After N=3, the time elapsed falls off rapidly and reaches a minimum
at approximately N=30. This must be the sweet spot between assigning and maintaining many locks and contention. As
expected, as N is increased further, the time elapsed increases, likely as a result of increased contention for locks.
Strangely, as N grows even larger, there appears to be a slight dip in the time elapsed; I am not exactly sure what causes
this; it could be due to stochastic fluctuations in the execution of the code.

I think the takeaway message here is that fine-grained and coarse-grained locking are rarely ideal; a medium-grained,
balanced locking technique is generally the best as illustrated by this plot.

b) Correlated

Intuitively, since the source and destination are less
than or equal to 10 indices apart, we expect that an N slightly greater than 10 would result in very little contention for locks; this should
provide a large speedup. We see this in the plot: the largest speedup occurs for an N larger than 10. It looks like
the maximum speedup occurs for N ~ 35. At larger N, the code again becomes slow. This is likely because at large N,
less elements can be swapped at once, as a single lock prevents many elements from swapping.
N ~ 35 must hit the sweet spot between avoiding correlation between the source and destination and not locking too indices
at once.
