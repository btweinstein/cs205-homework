These were all tested on my GPU:

GeForce GTX TITAN Black [Type: GPU ]
Maximum clock Frequency: 980 MHz
Maximum allocable memory size: 1610 MB
Maximum work group size 1024

# Part 1

## Maze 1
Finished after 915 iterations, 28.839424 ms total, 0.0315184961749 ms per iteration
Found 2 regions


## Maze 2
Finished after 532 iterations, 16.097408 ms total, 0.0302582857143 ms per iteration
Found 35 regions

# Part 2

## Maze 1
Finished after 529 iterations, 16.9664 ms total, 0.0320725897921 ms per iteration
Found 2 regions

## Maze 2
Finished after 274 iterations, 8.630432 ms total, 0.0314979270073 ms per iteration
Found 35 regions

It looks like the number of iterations was approximately halved. This makes sense;
if you look up your grandparents label, information can propagate at a rate of
two pixels per iteration as opposed to one. This should halve the number
of iterations.

# Part 3

## Maze 1
Finished after 10 iterations, 0.386208 ms total, 0.0386208 ms per iteration
Found 2 regions

## Maze 2
Finished after 9 iterations, 0.349952 ms total, 0.0388835555556 ms per iteration
Found 35 regions

This step has a drastic effect; the number of iterations are reduced by about a factor of
100 while the time per iteration stays about the same! The algorithm likely moved from O(N) to O(logN), drastically
increasing its speed.

# Part 4

## Maze 1
Finished after 10 iterations, 1.091232 ms total, 0.1091232 ms per iteration
Found 2 regions

## Maze 2

Finished after 9 iterations, 0.983008 ms total, 0.109223111111 ms per iteration
Found 35 regions

In this part, I have a single thread fetch grandparents. The thread saves the last fetch it performed to avoid
repeatedly reading the same global value within a workgroup. This code takes approximately 4 times longer to run per
iteration than part 3 where I did this step in parallel (with many threads).

In the parallel case, every thread will try to lookup a position in global memory simultaneously. If there are different
positions in global memory, they may be read approximately simultaneously via coalesced reads. However, if every thread
is trying to read the same piece of global memory, that piece will have to be read serially (unless the GPU has
hardware to handle this). If the GPU can read the memory in parallel regardless of repitition, multiple threads will
always be faster.

If there are, on average, Q different labels in a workgroup for N threads running in parallel, there will have to be
something like N/Q global reads, so the time elapsed in our algorithm will be (N/Q)*T where T is the time of a
global read.

In contrast, if only one thread is reading and caching its last lookup, there will be something like alpha*Q*T global
reads, where alpha takes into account how often the current fetch differs from the last.
So, in order for a single thread to be faster, we require

alpha*Q*T < (N/Q)*T
alpha*Q < (N/Q)

Therefore, if we want the single thread to win, we require, roughly,

(alpha*Q^2)/N < 1

So if Q^2/N is small, we expect the single thread to win, as long as switching between the next fetch (alpha) is not too
large. If Q^2/N is large, i.e. Qmax = N, we expect that the parallelized code will be faster.
In our program, Q^2/N must be large enough that the parallelized code still wins. If we could write code to reduce
alpha, i.e. caching more than one global value, or decrease the number of labels in a given workgroup while holding the
number of threads fixed (making Q^2/N smaller), the single thread could potentially win.

# Part 5

It still definitely works. It takes a couple more iterations, as the labels can increase and can be
overwritten while updating, but it converges to the correct answer. For example,

## Maze 1
Finished after 11 iterations, 1.244192 ms total, 0.113108363636 ms per iteration
Found 2 regions

## Maze 2
Finished after 10 iterations, 1.113504 ms total, 0.1113504 ms per iteration
Found 35 regions

The number of iterations varies as well, somewhere between 8 and 12.