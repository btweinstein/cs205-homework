#####################################################################
A) We do see some behavior that I would naively expect. We initiate
a pool of four processes and naively expect that the jobs
1-10 should complete in order. Instead what we see is something like:

Hi Job 0
Hi Job 2
Hi Job 3
Hi Job 1
Bye Job 0
Hi Job 4
Bye Job 3
Bye Job 2

Somehow, Job 2 is launched before job 3 *and* job 1! Then, even though
job 2 was launched before job 3, it finishes after job 3; I would not
expect this. Evidently it is not possible to control precisely when
each thread executes and finishes!

The inability to control exactly when threads execute and finish has
large implications for parallel computing. We cannot expect results
from our computations to be returned in the order we submitted the jobs;
we will consequently have to keep track of exactly what was run on each
thread. Also, if threads depend on results from other threads, we will
have to be very careful about when we execute the dependent threads; timing
will be a big issue. It may be hard to parallelize some tasks for this reason.

One scenario where this would be important is trying to have multiple
data-analyzing threads reading from a set of data that another thread is producing(
a data producing thread). If timing is not carefully controlled, the data-analyzing
threads may try to read a dataset that does not yet exist, causing problems.

#####################################
B) Naively, we expect that the serial time divided by the parallel time
should always be greater than one. Instead, at times less than roughly
$10^{-4}$, serial execution is *faster* than parallel execution! This is
likely because the overhead associated with setting up parallel pools and
keeping track of threads is greater than the time of running the burnTime program in this regime.
Evidently, it does not pay to run code in parallel if each job executes extremely quickly,
as the overhead associated with running a parallel program can ultimately result in a longer
execution time!